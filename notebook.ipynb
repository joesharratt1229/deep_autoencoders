{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1q/q1xm4z_j5cgdswpvb5n90pc00000gn/T/ipykernel_2656/3617104905.py:11: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # For export\n",
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "dataset_path = os.getcwd() + \"/data\"\n",
    "checkpoint_path = os.getcwd() + \"/saved_models\"\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "device = torch.device(\"mps:0\") if torch.backends.mps.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial9/cifar10_64.ckpt...\n",
      "Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial9/cifar10_128.ckpt...\n",
      "Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial9/cifar10_256.ckpt...\n",
      "Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial9/cifar10_384.ckpt...\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "# Github URL where saved models are stored for this tutorial\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial9/\"\n",
    "# Files to download\n",
    "pretrained_files = [\"cifar10_64.ckpt\", \"cifar10_128.ckpt\", \"cifar10_256.ckpt\", \"cifar10_384.ckpt\"]\n",
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in pretrained_files:\n",
    "    file_path = os.path.join(checkpoint_path, file_name)\n",
    "    if not os.path.isfile(file_path):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /Users/joesh/deep_autoencoders/data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e86e088dd64fe891e2522855e24e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/joesh/deep_autoencoders/data/cifar-10-python.tar.gz to /Users/joesh/deep_autoencoders/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset = CIFAR10(root = dataset_path, train = True, transform = transform, download = True)\n",
    "\n",
    "pl.seed_everything(42)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "\n",
    "# Loading the test set\n",
    "test_set = CIFAR10(root=dataset_path, train=False, transform=transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "train_loader = data.DataLoader(train_set, batch_size=256, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "val_loader = data.DataLoader(val_set, batch_size=256, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_loader = data.DataLoader(test_set, batch_size=256, shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "def get_train_images(num):\n",
    "    return torch.stack([train_dataset[i][0] for i in range(num)], dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the encoder. The encoder consists of a deep convolutional network where the image is scaled down layer by layer using strided convolutions. After the image is downscaled three times, the features are then flattened and linear layers are applied. \n",
    "\n",
    "Given small size of model neglect batch normalisation, as want encodings of each images to be independent of other images. Other normalisation techniques such as instance and layer normalisation can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_input_channels : int,\n",
    "                 base_channels_size: int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.GELU):\n",
    "                 \n",
    "                 super().__init__()\n",
    "                 c_hid = base_channels_size\n",
    "                 self.net = nn.Sequential(\n",
    "                    nn.Conv2d(num_input_channels, c_hid, kernel_size = 3, padding = 1, stride = 2),\n",
    "                    act_fn(),\n",
    "                    nn.Conv2d(c_hid, c_hid, kernel_size = 3, padding = 1), \n",
    "                    act_fn(),\n",
    "                    nn.Conv2d(c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 16x16 => 8x8\n",
    "                    act_fn(),\n",
    "                    nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "                    act_fn(),\n",
    "                    nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 8x8 => 4x4\n",
    "                    act_fn(),\n",
    "                    nn.Flatten(), # Image grid to single feature vector\n",
    "                    nn.Linear(2*16*c_hid, latent_dim)\n",
    "                 )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder is a mirrored, flipped version of the encoder but the strided convolutions are replaced by transposed convolutions to upscale features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "        def __init__(self,\n",
    "                 num_input_channels : int,\n",
    "                 base_channel_size : int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.GELU):\n",
    "\n",
    "                 super().__init__()\n",
    "                 c_hid = base_channel_size\n",
    "                 self.linear = nn.Sequential(\n",
    "                    nn.Linear(latent_dim, 2 * 16 * c_hid),\n",
    "                    act_fn()\n",
    "                 )\n",
    "                 \n",
    "                 self.net = nn.Sequential(\n",
    "                    nn.ConvTranspose2d(2 * c_hid, 2*c_hid, kernel_size=3, output_padding=1, padding=1, stride=2),\n",
    "                    act_fn(),\n",
    "                    nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding = 1),\n",
    "                    act_fn(),\n",
    "                    nn.ConvTranspose2d(2*c_hid, c_hid, kernel_size=3, output_padding = 1, padding = 1, stride = 2),\n",
    "                    act_fn(),\n",
    "                    nn.ConvTranspose2d(c_hid, c_hid, kernel_size=3, padding = 1),\n",
    "                    act_fn(),\n",
    "                    nn.ConvTranspose2d(c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2),\n",
    "                    nn.Tanh()\n",
    "                 )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.linear(x)\n",
    "            x = x.reshape(x.shape[0], -1, 4, 4)\n",
    "            x = self.net(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 base_channel_size: int,\n",
    "                 latent_dim: int,\n",
    "                 encoder_class : object = Encoder,\n",
    "                 decoder_class : object = Decoder,\n",
    "                 num_input_channels: int = 3,\n",
    "                 width: int = 32,\n",
    "                 height: int = 32):\n",
    "\n",
    "                 super().__init__()\n",
    "\n",
    "                 self.save_hyperparameters()\n",
    "\n",
    "                 self.encoder = encoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "                 self.decoder = decoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "\n",
    "                 self.example_input_array = torch.zeros(2, num_input_channels, width, height)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "    def _get_reconstruction_loss(self, batch):\n",
    "        \"\"\"\n",
    "        Given a btach of images, this function returns the reconstruction loss which is the MSE in\n",
    "        this instance\n",
    "        \"\"\"\n",
    "\n",
    "        x, _ = batch\n",
    "        x_hat = self.forward(x)\n",
    "        loss = F.mse_loss(x, x_hat, reduction='none')\n",
    "        loss = loss.sum( dim = [1,2,3]).mean(dim=[0]) #calculates the sum of the loss along the dimensions 1,2,3 (batch_size, loss) from there compute mean\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr = 1e-3)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor = 0.2,\n",
    "            patience = 20, \n",
    "            min_lr = 5e-5\n",
    "        )\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "    \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log('test_loss', loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cifar(latent_dim):\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(checkpoint_path, f\"cifar10_{latent_dim}\"),\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"mps\") else \"cpu\",\n",
    "                         devices = 1,\n",
    "                         max_epochs = 500,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True),\n",
    "                         LearningRateMonitor(\"epoch\")])\n",
    "    \n",
    "    trainer.logger._log_graph = True\n",
    "    trainer.logger._default_hp_metric = None\n",
    "\n",
    "    pretrained_filename = os.path.join(checkpoint_path, f\"cifar10_{latent_dim}.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = AutoEncoder.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        model = AutoEncoder(base_channel_size=32, latent_dim=latent_dim)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    val_result = trainer.test(model, val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_loader, verbose=False)\n",
    "    result = {\"test\": test_result, \"val\": val_result}\n",
    "    return model, result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /Users/joesh/deep_autoencoders/saved_models/cifar10_64/lightning_logs\n",
      "\n",
      "  | Name    | Type    | Params | In sizes       | Out sizes     \n",
      "----------------------------------------------------------------------\n",
      "0 | encoder | Encoder | 168 K  | [2, 3, 32, 32] | [2, 64]       \n",
      "1 | decoder | Decoder | 168 K  | [2, 64]        | [2, 3, 32, 32]\n",
      "----------------------------------------------------------------------\n",
      "337 K     Trainable params\n",
      "0         Non-trainable params\n",
      "337 K     Total params\n",
      "1.348     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5926a82dc85e4c3197dadfc8eb3fa942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f33b223e4434034b5708309496d4851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2f30ee1ea145bd80350f2453dd60d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb2dec1a1fd481581d5929c9c572bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_dict = {}\n",
    "for latent_dim in [64, 128, 256, 384]:\n",
    "    model_ld, result_ld = train_cifar(latent_dim)\n",
    "    model_dict[latent_dim] = {\"model\": model_ld, \"result\": result_ld}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
